---
title: "Quick Start"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Quick Start}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## What is BORG?

BORG (Bounded Outcome Risk Guard) detects when model evaluation is compromised
by information leakage between training and test data. It identifies violations
that inflate performance estimates and blocks invalid evaluations.

## The Problem

Consider a common machine learning workflow:

```r
# Typical workflow (PROBLEMATIC)
data <- read.csv("my_data.csv")

# Preprocessing on FULL data - LEAKS TEST INFO
data_scaled <- scale(data)

# Split AFTER preprocessing
train_idx <- 1:800
test_idx <- 801:1000
train <- data_scaled[train_idx, ]
test <- data_scaled[test_idx, ]

# Model training and evaluation
model <- train_model(train)
performance <- evaluate(model, test)  # Inflated!
```

The problem: `scale()` computed mean and standard deviation using ALL data,
including the test set. This means:

- Training data contains test set statistics
- Test performance is artificially inflated
- Results won't generalize to truly unseen data

## The Solution

Use `borg()` to validate any object:

```r
library(BORG)

# Check a train/test split
borg(data, train_idx = 1:800, test_idx = 801:1000)

# Check a preprocessing object
borg(my_recipe, train_idx, test_idx, data = data)

# Check a fitted model
borg(my_model, train_idx, test_idx, data = data)
```

`borg()` auto-detects the object type and runs the appropriate validation. If
something is wrong, it errors with a clear message:

```r
borg(data, train_idx = 1:800, test_idx = 801:1000)
# Error: BORG HARD VIOLATION: preprocessing fitted on full data
```

## Other Functions

| Function | Use Case |
|----------|----------|
| `borg()` | Validate any object |
| `borg_inspect()` | Detailed object inspection with risk report |
| `borg_validate()` | Validate a complete workflow |
| `borg_rewrite()` | Attempt automatic repair |

### Detailed Inspection

`borg()` returns a risk report:

```r
result <- borg(my_recipe, train_idx, test_idx, data = data)

result@is_valid      # TRUE/FALSE
result@n_hard        # Count of hard violations
result@n_soft        # Count of soft warnings
as.data.frame(result)  # Tabular risk summary
```

## Risk Classification

BORG classifies risks into two categories:

### Hard Violations (Block)

These invalidate evaluation entirely:

- **Index overlap**: Same rows in train and test
- **Preprocessing leak**: Normalization/imputation fitted on full data
- **Target leakage**: Features derived from the outcome
- **Temporal look-ahead**: Using future information for past predictions

### Soft Inflation (Warn)

These bias results but may preserve model ranking:

- **Insufficient spatial blocking**: Block size smaller than autocorrelation range
- **Post-hoc subgroup analysis**: Discovering subgroups on test data
- **Proxy leakage**: Features suspiciously correlated with target

## Common Patterns

### Cross-Validation

```r
# BAD: preprocessing before CV
data_scaled <- scale(data)
cv_results <- cross_validate(data_scaled)  # Leaky!

# GOOD: check with borg first
borg(data, train_idx, test_idx)
# Preprocessing must happen per-fold
```

### Temporal Splits

```r
# Validate temporal ordering
borg(ts_data, train_idx = 1:800, test_idx = 801:1000, temporal_col = "date")
```

### Grouped Data

```r
# Ensure no patient appears in both sets
borg(patient_data, train_idx, test_idx, group_col = "patient_id")
```

## Next Steps

- [Risk Taxonomy](risk-taxonomy.html): Full catalog of detectable risks
- [Framework Integration](frameworks.html): Using BORG with caret, tidymodels, mlr3
